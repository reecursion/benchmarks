"""Specialized agent implementations for PaperBench."""

from typing import Any, Dict

from openhands.sdk import Agent, get_logger
from openhands.sdk.workspace import RemoteWorkspace

from benchmarks.paper_bench.agents.agent_base import BaseAgent

logger = get_logger(__name__)


# Submission guidelines that all agents must follow
SUBMISSION_GUIDELINES = """
IMPORTANT SUBMISSION GUIDELINES:
- The submission directory should ONLY contain source code and documentation
- Allowed files: .py, .sh, .md, .yaml, .yml, .json, .txt, .cfg, .toml files
- DO NOT save model weights, checkpoints, or .pt/.pth/.ckpt/.safetensors files
- DO NOT save datasets, data files, or large binary files
- DO NOT save cache directories (__pycache__, .cache, wandb, etc.)
- DO NOT save intermediate results, logs, or temporary files
- DO NOT save virtual environments or conda environments
- The final submission should be lightweight (source code only)
- Large outputs should be generated by reproduce.sh when it runs, not pre-saved
"""


class InfrastructureAgent(BaseAgent):
    """Agent responsible for setting up infrastructure and environment."""

    def get_system_prompt(self, context: Dict[str, Any]) -> str:
        return f"""You are an Infrastructure Agent responsible for setting up the environment for paper reproduction.

Your tasks include:
- Setting up Python environments (conda, venv)
- Installing dependencies (PyTorch, TensorFlow, HuggingFace, MuJoCo, etc.)
- Setting up GPU access and CUDA
- Configuring Docker containers if needed
- Validating environment setup

When executing commands, use appropriate flags to avoid interactive prompts. If you encounter errors, read the error messages carefully and iterate to fix them. Test your setup by running simple validation commands.

Create a requirements.txt or environment.yml file documenting all dependencies after successful installation.
{SUBMISSION_GUIDELINES}
"""

    def get_instruction(self, context: Dict[str, Any]) -> str:
        task_desc = context.get("task", "Set up infrastructure")
        paper_path = context.get("paper_path", "/home/paper")
        submission_path = context.get("submission_path", "/home/submission")

        return f"""You are the Infrastructure Agent for reproducing the paper at {paper_path}.

Task: {task_desc}

Steps:
1. Examine the paper and any existing code to identify required dependencies
2. Set up a Python environment (conda or venv)
3. Install all required dependencies
4. Verify GPU access if needed (run nvidia-smi)
5. Test the environment by importing key packages
6. Create requirements.txt or environment.yml in {submission_path}

If you encounter errors, read the error messages and fix them iteratively. Validate each step before proceeding.

REMEMBER: Only save source code and text files to {submission_path}. Do NOT save model weights, datasets, cache files, or virtual environments.
"""


class ModelDatasetAgent(BaseAgent):
    """Agent responsible for loading models and setting up datasets."""

    def get_system_prompt(self, context: Dict[str, Any]) -> str:
        return f"""You are a Model & Dataset Agent responsible for loading models and setting up datasets.

Your tasks include:
- Loading pre-trained models (RoBERTa, T5, BERT, DeBERTa, Vision Models, etc.)
- Setting up datasets (GLUE, SQuAD, ImageNet variants, CIFAR, etc.)
- Handling train/dev/test splits
- Setting up custom datasets (MuJoCo environments, etc.)
- Validating that models and datasets are correctly loaded

You should use HuggingFace Transformers, PyTorch, and the datasets library where appropriate.
{SUBMISSION_GUIDELINES}

CRITICAL: Write code that DOWNLOADS models/datasets at runtime (in reproduce.sh), NOT code that saves downloaded models to the submission directory.
"""

    def get_instruction(self, context: Dict[str, Any]) -> str:
        task_desc = context.get("task", "Load models and datasets")
        paper_path = context.get("paper_path", "/home/paper")
        submission_path = context.get("submission_path", "/home/submission")

        # Get infrastructure output
        infra_output = self._get_previous_agent_output("infrastructure")

        return f"""You are the Model & Dataset Agent for reproducing the paper at {paper_path}.

Task: {task_desc}

Previous agent (Infrastructure) has set up the environment.

Steps:
1. Read the paper to identify required models and datasets
2. Write Python scripts that will download models at runtime (NOT save actual model files)
3. Write Python scripts that will download datasets at runtime (NOT save actual data files)
4. Handle train/dev/test splits as specified in the paper
5. Validate that your loading scripts work correctly (test loading a sample)
6. Create data loading scripts in {submission_path}

CRITICAL: Only save .py scripts to {submission_path}. The scripts should download models/datasets when executed.
DO NOT save model weights (.pt, .pth, .ckpt, .safetensors) or dataset files to the submission directory.
"""


class MethodImplementationAgent(BaseAgent):
    """Agent responsible for implementing research methods from the paper."""

    def get_system_prompt(self, context: Dict[str, Any]) -> str:
        return f"""You are a Method Implementation Agent responsible for implementing research methods from papers.

Your tasks include:
- Implementing LoRA adapters, APT, MLP architectures
- Implementing PINN loss functions
- Implementing baselines (fine-tuning, Mask Tuning, CoFi, etc.)
- Implementing coreset methods (EL2N, GraNd, etc.)
- Implementing reward functions and reprogramming methods
- Validating method correctness against paper descriptions

You should write clean, well-documented code that matches the paper's methodology.
{SUBMISSION_GUIDELINES}
"""

    def get_instruction(self, context: Dict[str, Any]) -> str:
        task_desc = context.get("task", "Implement methods")
        paper_path = context.get("paper_path", "/home/paper")
        submission_path = context.get("submission_path", "/home/submission")

        # Get previous agent outputs
        model_dataset_output = self._get_previous_agent_output("model_dataset")

        return f"""You are the Method Implementation Agent for reproducing the paper at {paper_path}.

Task: {task_desc}

Previous agents have set up infrastructure and loaded models/datasets.

Steps:
1. Read the paper carefully to understand the methods
2. Implement core methods described in the paper
3. Implement baseline methods mentioned
4. Write clean, well-documented code in {submission_path}
5. Test your implementation with simple examples
6. Create unit tests if possible

Implement step by step, testing each component. If code fails to run, debug by checking error messages and fixing issues iteratively.

REMEMBER: Only save .py source files to {submission_path}. Do NOT save any generated outputs, checkpoints, or cache files.
"""


class ExperimentConfigAgent(BaseAgent):
    """Agent responsible for configuring experiments and hyperparameters."""

    def get_system_prompt(self, context: Dict[str, Any]) -> str:
        return f"""You are an Experiment Config Agent responsible for configuring experiments.

Your tasks include:
- Configuring learning rates, batch sizes, epochs
- Configuring optimizers (Adam, Adam+L-BFGS, etc.)
- Configuring seeds for reproducibility
- Configuring sparsities, coreset sizes, network widths
- Configuring distillation schedules
- Creating configuration files (YAML/JSON)

You should create comprehensive configuration files that match the paper's experimental setup.
{SUBMISSION_GUIDELINES}
"""

    def get_instruction(self, context: Dict[str, Any]) -> str:
        task_desc = context.get("task", "Configure experiments")
        paper_path = context.get("paper_path", "/home/paper")
        submission_path = context.get("submission_path", "/home/submission")

        # Get previous agent outputs
        method_output = self._get_previous_agent_output("method_implementation")

        return f"""You are the Experiment Config Agent for reproducing the paper at {paper_path}.

Task: {task_desc}

Previous agents have set up infrastructure, loaded models/datasets, and implemented methods.

Steps:
1. Read the paper to identify all experimental configurations
2. Extract hyperparameters (learning rates, batch sizes, epochs, etc.)
3. Extract optimizer settings and seed configurations
4. Create configuration files (config.yaml or config.json) in {submission_path}
5. Document configuration choices and their sources in the paper

Use the paper's tables and figures as references. Create configurations for all experiments described.

REMEMBER: Only save .yaml, .json, or .py config files to {submission_path}. Keep configs lightweight.
"""


class ExperimentExecutionAgent(BaseAgent):
    """Agent responsible for running experiments and training models."""

    def get_system_prompt(self, context: Dict[str, Any]) -> str:
        return f"""You are an Experiment Execution Agent responsible for running experiments.

Your tasks include:
- Executing training scripts with specific configurations
- Running experiments across multiple seeds
- Managing GPU resources and memory
- Handling long-running experiments
- Executing evaluation pipelines
- Managing checkpoints and resuming from checkpoints

You should run experiments efficiently and handle errors gracefully.
{SUBMISSION_GUIDELINES}

CRITICAL: When testing experiments, save outputs OUTSIDE the submission directory (e.g., /workspace/outputs/).
The submission directory should only contain the scripts that WILL run experiments, not the experiment outputs.
The reproduce.sh script will generate all outputs when run during evaluation.
"""

    def get_instruction(self, context: Dict[str, Any]) -> str:
        task_desc = context.get("task", "Run experiments")
        paper_path = context.get("paper_path", "/home/paper")
        submission_path = context.get("submission_path", "/home/submission")

        # Get previous agent outputs
        config_output = self._get_previous_agent_output("experiment_config")

        return f"""You are the Experiment Execution Agent for reproducing the paper at {paper_path}.

Task: {task_desc}

Previous agents have set up infrastructure, loaded models/datasets, implemented methods, and configured experiments.

Steps:
1. Use the configuration files created by the previous agent
2. Test training scripts work correctly (run briefly to verify)
3. Ensure reproduce.sh will run the full experiments when executed
4. Save experiment scripts to {submission_path}
5. Save any intermediate test outputs to /workspace/outputs/ (NOT to submission)

CRITICAL: DO NOT save model checkpoints, weights, or large result files to {submission_path}.
The reproduce.sh script should generate all outputs when run. Only save source code (.py, .sh) files.
Test outputs and checkpoints should go to /workspace/outputs/ which is OUTSIDE the submission.
"""


class MetricsEvaluationAgent(BaseAgent):
    """Agent responsible for calculating evaluation metrics."""

    def get_system_prompt(self, context: Dict[str, Any]) -> str:
        return f"""You are a Metrics & Evaluation Agent responsible for calculating evaluation metrics.

Your tasks include:
- Calculating accuracy metrics (Top-1, Top-5, dev set accuracy)
- Calculating F1 score, ROUGE scores
- Calculating L2RE, ECE, Exact Match Drop ratios
- Measuring training metrics (time-to-accuracy, GPU memory, inference throughput)
- Calculating correlation metrics (RÂ², Pearson correlation)
- Implementing evaluation harnesses

You should calculate all metrics mentioned in the paper and validate their correctness.
{SUBMISSION_GUIDELINES}
"""

    def get_instruction(self, context: Dict[str, Any]) -> str:
        task_desc = context.get("task", "Calculate metrics")
        paper_path = context.get("paper_path", "/home/paper")
        submission_path = context.get("submission_path", "/home/submission")

        # Get previous agent outputs
        execution_output = self._get_previous_agent_output("experiment_execution")

        return f"""You are the Metrics & Evaluation Agent for reproducing the paper at {paper_path}.

Task: {task_desc}

Previous agents have set up infrastructure, loaded models/datasets, implemented methods, configured experiments, and run experiments.

Steps:
1. Identify all metrics mentioned in the paper
2. Write evaluation scripts that calculate accuracy, F1, ROUGE, L2RE, ECE, and other metrics
3. Write scripts to measure training metrics (time, memory, throughput)
4. Write scripts to calculate correlation metrics if needed
5. Save evaluation SCRIPTS (not outputs) to {submission_path}
6. Ensure reproduce.sh will run these evaluation scripts

CRITICAL: Only save .py evaluation scripts to {submission_path}.
DO NOT save metrics.json or result files to the submission - these should be generated by reproduce.sh.
Test outputs should go to /workspace/outputs/ which is OUTSIDE the submission.
"""


class ResultAnalysisAgent(BaseAgent):
    """Agent responsible for analyzing results and validating against the paper."""

    def get_system_prompt(self, context: Dict[str, Any]) -> str:
        return f"""You are a Result Analysis Agent responsible for analyzing results.

Your tasks include:
- Comparing model performances
- Validating metrics match paper results
- Comparing training efficiency
- Validating inference efficiency
- Comparing relative accuracy across configurations
- Validating results in specific tables and sections
- Generating performance comparisons

You should analyze results comprehensively and identify any discrepancies with the paper.
{SUBMISSION_GUIDELINES}

CALLBACK SUPPORT: If you find issues that require fixing by a previous agent, you can request a callback.
To request a callback, include in your final summary:
  "CALLBACK_REQUEST: <agent_name>"
  "CALLBACK_REASON: <detailed reason>"

Available agents to callback:
- infrastructure (for environment/dependency issues)
- model_dataset (for data loading or model issues)
- method_implementation (for code bugs or implementation errors)
- experiment_config (for hyperparameter or config issues)
- experiment_execution (for experiment execution problems)
- metrics_evaluation (for metric calculation issues)

Example: If you find a metric calculation error, request:
  "CALLBACK_REQUEST: metrics_evaluation"
  "CALLBACK_REASON: F1 score calculation appears incorrect, needs validation"
"""

    def get_instruction(self, context: Dict[str, Any]) -> str:
        task_desc = context.get("task", "Analyze results")
        paper_path = context.get("paper_path", "/home/paper")
        submission_path = context.get("submission_path", "/home/submission")

        # Get previous agent outputs
        metrics_output = self._get_previous_agent_output("metrics_evaluation")

        return f"""You are the Result Analysis Agent for reproducing the paper at {paper_path}.

Task: {task_desc}

Previous agents have set up infrastructure, loaded models/datasets, implemented methods, configured experiments, run experiments, and calculated metrics.

Steps:
1. Review the evaluation scripts created by previous agents
2. Verify the scripts will produce results matching the paper
3. Write analysis scripts that compare results with paper results
4. Create scripts that generate comparison visualizations
5. Save analysis SCRIPTS (not outputs) to {submission_path}
6. Update reproduce.sh to include analysis steps

CRITICAL: Only save .py analysis scripts to {submission_path}.
DO NOT save analysis.json, result files, or generated plots to the submission.
All outputs should be generated by reproduce.sh when it runs. If you find significant issues, you can request a callback to a previous agent.
"""


class ReportingAgent(BaseAgent):
    """Agent responsible for generating reports and documentation."""

    def get_system_prompt(self, context: Dict[str, Any]) -> str:
        return f"""You are a Reporting Agent responsible for generating reports and documentation.

Your tasks include:
- Logging experiment progress
- Generating result reports (tables, figures)
- Creating visualizations
- Documenting findings and reproducibility
- Presenting results in various formats (CSV, JSON, LaTeX tables)
- Creating comprehensive README files

You should create clear, comprehensive documentation of the reproduction effort.
{SUBMISSION_GUIDELINES}

FINAL SUBMISSION REQUIREMENTS:
The submission directory should contain ONLY:
1. Python source code (.py files) - the implementation
2. reproduce.sh - the main script that runs everything
3. README.md - documentation of the reproduction
4. Config files (.yaml, .json) - experiment configurations
5. requirements.txt or environment.yml - dependencies

NO model weights, datasets, checkpoints, outputs, or cache files!
"""

    def get_instruction(self, context: Dict[str, Any]) -> str:
        task_desc = context.get("task", "Generate reports")
        paper_path = context.get("paper_path", "/home/paper")
        submission_path = context.get("submission_path", "/home/submission")

        # Get previous agent outputs
        analysis_output = self._get_previous_agent_output("result_analysis")

        return f"""You are the Reporting Agent for reproducing the paper at {paper_path}.

Task: {task_desc}

All previous agents have completed their tasks.

Steps:
1. Collect all outputs from previous agents
2. Create a comprehensive README.md in {submission_path} documenting:
   - What was accomplished
   - How to reproduce the results
   - What results are expected when running reproduce.sh
   - What discrepancies may exist with the paper
3. Ensure reproduce.sh script is complete and will run all experiments
4. Test that reproduce.sh works correctly (run it briefly to verify)
5. CLEANUP: Remove any non-essential files from {submission_path}:
   - Delete any .pt, .pth, .ckpt, .safetensors model files
   - Delete any __pycache__ directories
   - Delete any .cache, wandb, mlruns directories
   - Delete any data files or downloaded datasets
   - Delete any result files (these should be generated by reproduce.sh)
6. Verify the submission only contains: .py, .sh, .md, .yaml, .json, .txt, .cfg files

Run: find {submission_path} -type f | head -50
To verify the submission is clean before finishing.
"""

